---
title: "Practical Machine Learning Course Project"
author: "Vladimir Mijatovic"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Predicting how well the exercise is performed

The main goal of the project is the analysis of the manner of how well the exercise is performed.  

### Background 
Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. 
In this particular datased, a group of 6 participants have collected measures from various sensors while doing barbell lifts correctly and incorrectly.  
More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

Correct performance is marked with "Class A", while other classes are incorrect exercise (with mistakes).

### Prepare the Environment

Let's prepare the environment

```{r echo=FALSE, message=FALSE}
#setup for the needed libraries
library(tidyverse)
library(nnet)
library(caret)
library(corrplot)
library(rattle)
set.seed(22822)
```

## Load and Prepare Data

First let's load and prepare the data

```{r load_data, echo=FALSE}

trainData <- read.csv("./_data/pml-training.csv", 
                      header = T, 
                      check.names = F,
                      stringsAsFactors = F)

testData <- read.csv("./_data/pml-testing.csv",
                     header = T,
                     check.names = F,
                     stringsAsFactors = F)

paste0("Dimensions of data are: ", dim(testData))
```

We can see that test data has only `20` observations.  It is better therefore to call this data `validationData`, and to partition _trainData_ into two groups.  But before partitioning we need to do some cleaning. 

```{r}
validationData <- testData
testData <- NULL
```


# Exploratory Data Analisys

Let's see all the data fields

```{r}
str(trainData, give.head = TRUE)
```


Let's remove unnecessary variables.  Variables that have a lot of NA should be removed.  How many variables have lot of NAs?

```{r}

na_count <-sapply(trainData, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
sum(na_count > 0)
```

Seems like there are columns that are only NAs.  These should be removed, as they can't contribute to our prediction model. 

```{r}
#get rid of columns where NAs are more than 90%
trainData <- trainData[, colMeans(is.na(trainData)) < 0.9]
validationData <- validationData[, colMeans(is.na(validationData)) < 0.9]

# get rid of columns that don't contribute (e.g. timestamp, name, etc)
trainData <- trainData[, -c(1:7)]
validationData <- validationData[, -c(1:7)]

```

### Remove Variables With Near-Zero Variance

We should remove variables that are having Near-Zero Variance, as they don't contribute to our model either. 

```{r near_zero_variance_variables, cache=TRUE}
near_zero_variables <- nearZeroVar(trainData)
```

These are variables that should be removed: 
``` {r }
names(trainData)[near_zero_variables]
```

```{r remove_near_zero_variables}
trainData <- trainData[, -near_zero_variables]

```



### Partitioning 
Now let's partition training dataset.  We partition it in 70% training and 30% testing set:

```{r partitioning_dataset}
inTrain <- createDataPartition(trainData$classe, 
                               p = 0.7, 
                               list = FALSE)

training_set <- trainData[inTrain, ]
testing_set <- trainData[-inTrain, ]

#check their dimensions
dim(training_set)

dim(testing_set)

```

## Correlation Analysis

Let's examine the correlation among those variable. 

```{r out.width="90%"}
corr_matrix <- cor(training_set[, -53])
corrplot(corr_matrix, 
         type = "lower",
         tl.cex = 0.6,
         tl.col = rgb(0, .5, .1))
```


## Prediction Model Building

First let's make response variable `classe` into ordered factor variable

```{r order_response_variable}
# make it into factor and relevel (so that "A" is reference level)
training_set$classe <- factor(training_set$classe)
training_set$classe <- relevel(training_set$classe, 
                            ref = "A")

testing_set$classe <- factor(testing_set$classe)
testing_set$classe <- relevel(testing_set$classe, 
                            ref = "A")
```


Finally we can train our models and cross-validate.  In order to estimate which model performs the best, we will use 3 different ML algorithms, and cross-validate to see their performance.

### Random Forest

Let's train the model. We use in train control cross-validation method to avoid overfitting.   We use 5-fold cross-validation. 


```{r model_randomForest, cache=TRUE, warning=FALSE, message=FALSE}

model_rf <- train(classe ~ .,
                  method = "rf",
                  data = training_set,
                  trControl = trainControl(
                          method = "cv",
                          number = 5),
                  allowParallel = TRUE)

model_rf$finalModel

```





Let's see how it performs on `testing_set` 

```{r }

prediction_rf <- predict(model_rf,
                         newdata = testing_set)

conf_mat_rf <- confusionMatrix(prediction_rf, testing_set$classe)
conf_mat_rf

```

The accuracy is `r conf_mat_rf$overall['Accuracy']`


### XGBoost

XGBoost has shown to be one of the most successful machine learning algorithms.  Let's see how it will work here.
We build model, as always, on training_set, using 5-fold cross-validation to reduce overfitting.

```{r model_xgboost, cache=TRUE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
model_xgboost <- train(classe ~ .,
                  method = "xgbTree",
                  data = training_set,
                  trControl = trainControl(
                          method = "cv", 
                          number = 5
                  ))

model_xgboost$finalModel
```

Let's see how it performs on `testing_set` 

```{r}

prediction_xgboost <- predict(model_xgboost,
                         newdata = testing_set)

conf_mat_xgboost <- confusionMatrix(prediction_xgboost, testing_set$classe)
conf_mat_xgboost
```

It accuracy is `r conf_mat_xgboost$overall['Accuracy']`


### Decision Tree

Now we should try with simple Decision Tree model, also using 5-fold cross-validation to avoid overfitting.

```{r model_decisionTree, cache=TRUE, out.width="90%", warning=FALSE}
train_control <- trainControl(method="cv", number=5, verboseIter=F)

model_trees <- train(classe ~ ., 
                   method = "rpart",
                   trControl = train_control,
                   data = training_set)

rattle::fancyRpartPlot(model_trees$finalModel,
                       sub = "Decision Tree")
```
Let's see how it performs on `testing_set` 

```{r}

prediction_trees <- predict(model_trees,
                         newdata = testing_set)

conf_mat_rpart <- confusionMatrix(prediction_trees, testing_set$classe)
conf_mat_rpart

```

Accuracy of simple Decision Tree model is only `r conf_mat_rpart$overall['Accuracy']`, which is clearly inferior to Random Forest and XGBoost.


# Conclusion

From the accuracy point of view we have the following results (out-of-sample accuracy):

1. Random Forest: **`r conf_mat_rf$overall['Accuracy']`**
2. XGBoost: **`r conf_mat_xgboost$overall['Accuracy']` **
3. Decision Tree (rpart): **`r conf_mat_rpart$overall['Accuracy']`**



# Predicting on validationData

Let's use XGBoost to predict on validation data:

```{r}


prediction_xgboost_validation <- predict(model_xgboost,
                                    newdata = validationData)



paste0("XGBoost: ", prediction_xgboost_validation)



```


